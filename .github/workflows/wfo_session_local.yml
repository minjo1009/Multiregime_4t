name: WFO All-in-One V2.1.3 (hardened-harvest)

on:
  workflow_dispatch:
    inputs:
      PARAMS_FILE:
        description: "params path (dot preferred, fallback underscore)"
        required: false
        default: "conf/params.v2.yml"
      DATA_ZIP:
        description: "data zip at repo root (optional)"
        required: false
        default: "ETHUSDT_1min_2020_2025.zip"
      CSV_GLOB:
        description: "csv glob override (if no zip)"
        required: false
        default: "**/*ETHUSDT*1min*2020*2025*.csv"
      ENTRYPOINTS:
        description: "python entrypoints (| separated)"
        required: false
        default: "run_4u.py|backtest/run_4u.py|run.py|backtest/runner.py"
      PY_VERSION:
        description: "python version"
        required: false
        default: "3.11"
      RUN_SINGLE:
        description: "also run single baseline? (true/false)"
        required: false
        default: "true"
      GRID_THR:
        description: "override thr list (comma)"
        required: false
        default: ""
      GRID_HOLD:
        description: "override hold list (comma)"
        required: false
        default: ""
      GRID_FILTER:
        description: "override filter list (comma)"
        required: false
        default: ""
      BUNDLE_NAME:
        description: "final single zip name (no .zip)"
        required: false
        default: "WFO_results_all"

jobs:
  prep:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.mk.outputs.matrix }}
      csv_glob: ${{ steps.mk.outputs.csv_glob }}
      params_used: ${{ steps.params_out.outputs.PF }}
    steps:
      - name: Checkout (pinned)
        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8
      - name: Setup Python (pinned)
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065
        with:
          python-version: ${{ github.event.inputs.PY_VERSION }}
      - name: Deps
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install pandas numpy pyyaml
      - name: Resolve params (dot -> underscore fallback)
        id: params_out
        run: |
          set -euo pipefail
          PF="${{ github.event.inputs.PARAMS_FILE }}"
          if [ ! -f "$PF" ] && [ "$PF" = "conf/params.v2.yml" ] && [ -f "conf/params_v2.yml" ]; then
            PF="conf/params_v2.yml"
          fi
          if [ ! -f "$PF" ]; then
            echo "Error: params file not found (conf/params.v2.yml or conf/params_v2.yml)"; exit 3
          fi
          echo "PF=$PF" >> "$GITHUB_OUTPUT"
      - name: Build matrix (+ overrides)
        id: mk
        env:
          PF: ${{ steps.params_out.outputs.PF }}
          CSV_GLOB_IN: ${{ github.event.inputs.CSV_GLOB }}
          GRID_THR: ${{ github.event.inputs.GRID_THR }}
          GRID_HOLD: ${{ github.event.inputs.GRID_HOLD }}
          GRID_FILTER: ${{ github.event.inputs.GRID_FILTER }}
        run: |
          set -euo pipefail
          : > _mk.py
          echo "import os,yaml,json" >> _mk.py
          echo "pf=os.environ['PF']" >> _mk.py
          echo "p=yaml.safe_load(open(pf,'r',encoding='utf-8'))" >> _mk.py
          echo "csvg=os.environ.get('CSV_GLOB_IN') or p.get('csv_glob','**/*.csv')" >> _mk.py
          echo "thr=(p.get('gate',{}) or {}).get('thr',[3.2,3.8])" >> _mk.py
          echo "hold=p.get('hold',[6,8])" >> _mk.py
          echo "filt=p.get('filter',['ema','none'])" >> _mk.py
          echo "def ov(env,cur,cast=float):" >> _mk.py
          echo "  v=os.environ.get(env,'').strip()" >> _mk.py
          echo "  if not v: return cur" >> _mk.py
          echo "  xs=[t.strip() for t in v.split(',') if t.strip()]" >> _mk.py
          echo "  if env!='GRID_FILTER':" >> _mk.py
          echo "    try: xs=[cast(x) for x in xs]" >> _mk.py
          echo "    except Exception: pass" >> _mk.py
          echo "  return xs or cur" >> _mk.py
          echo "thr=ov('GRID_THR',thr,float)" >> _mk.py
          echo "hold=ov('GRID_HOLD',hold,int)" >> _mk.py
          echo "filt=ov('GRID_FILTER',filt,str)" >> _mk.py
          echo "open('matrix.json','w',encoding='utf-8').write(json.dumps({'thr':thr,'hold':hold,'filter':filt}))" >> _mk.py
          echo "open('csv_glob.txt','w',encoding='utf-8').write(str(csvg))" >> _mk.py
          python _mk.py
          echo "matrix=$(cat matrix.json)" >> "$GITHUB_OUTPUT"
          echo "csv_glob=$(cat csv_glob.txt)" >> "$GITHUB_OUTPUT"

  single:
    if: ${{ github.event.inputs.RUN_SINGLE == 'true' }}
    needs: prep
    runs-on: ubuntu-latest
    steps:
      - name: Checkout (pinned)
        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8
      - name: Setup Python (pinned)
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065
        with:
          python-version: ${{ github.event.inputs.PY_VERSION }}
      - name: Deps
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install pandas numpy pyyaml
      - name: Unzip data if present
        run: |
          set -euo pipefail
          if [ -f "${{ github.event.inputs.DATA_ZIP }}" ]; then unzip -o "${{ github.event.inputs.DATA_ZIP }}" || true; fi
      - name: Preflight (auto-detect new/legacy)
        run: |
          set -euo pipefail
          PFS=""
          if [ -f "ci/preflight_strict.py" ]; then
            PFS="ci/preflight_strict.py"
          elif [ -f "preflight_strict.py" ]; then
            PFS="preflight_strict.py"
          fi
          if [ -n "${PFS}" ]; then
            if grep -q -- "--data-root" "${PFS}"; then
              mkdir -p _preflight_out
              python "${PFS}" \
                --data-root "${GITHUB_WORKSPACE}" \
                --csv-glob "${{ needs.prep.outputs.csv_glob }}" \
                --outdir "_preflight_out"
            else
              python "${PFS}" "${{ needs.prep.outputs.csv_glob }}"
            fi
          else
            : > _pre.py
            echo "import sys,glob,pandas as pd" >> _pre.py
            echo "g=sys.argv[1]" >> _pre.py
            echo "paths=glob.glob(g,recursive=True)" >> _pre.py
            echo "assert paths, f'No CSV found by glob: {g}'" >> _pre.py
            echo "df=pd.read_csv(paths[0],nrows=5)" >> _pre.py
            echo "req=['open_time','open','high','low','close','volume']" >> _pre.py
            echo "miss=[c for c in req if c not in df.columns]" >> _pre.py
            echo "assert not miss, f'Missing columns: {miss} in {paths[0]}'" >> _pre.py
            python _pre.py "${{ needs.prep.outputs.csv_glob }}"
          fi
      - name: Run single (entrypoint scan)
        env:
          ENTRYPOINTS: ${{ github.event.inputs.ENTRYPOINTS }}
        run: |
          set -euo pipefail
          IFS='|' read -ra EPS <<< "${ENTRYPOINTS}"
          found=0
          for ep in "${EPS[@]}"; do
            if [ -f "$ep" ]; then
              set +e
              python "$ep"
              rc=$?
              set -e
              if [ $rc -eq 0 ]; then found=1; break; fi
            fi
          done
          if [ $found -eq 0 ]; then echo "Warning: no entrypoint succeeded" >&2; fi
      - name: HARVEST single -> standardize & zip
        env:
          PF: ${{ needs.prep.outputs.params_used }}
        run: |
          set -euo pipefail
          OUT="out_single"
          mkdir -p "$OUT"
          # find & copy (first match wins)
          fcopy(){ p="$1"; dst="$2"; set +e; X=$(find . -type f -iname "$p" | head -n1); set -e; if [ -n "$X" ]; then cp -f "$X" "$OUT/$dst"; fi; }
          fcopy "trades.csv" "trades.csv"
          fcopy "*trade*.csv" "trades.csv"
          fcopy "summary.json" "summary.json"
          fcopy "*summary*.json" "summary.json"
          fcopy "preds_test.csv" "preds_test.csv"
          fcopy "*pred*.csv" "preds_test.csv"
          fcopy "gating_debug.json" "gating_debug.json"
          fcopy "*gating*.json" "gating_debug.json"
          # fallbacks if missing
          [ -f "$OUT/summary.json" ] || echo "{}" > "$OUT/summary.json"
          [ -f "$OUT/gating_debug.json" ] || echo "{}" > "$OUT/gating_debug.json"
          [ -f "$OUT/preds_test.csv" ] || echo "empty" > "$OUT/preds_test.csv"
          [ -f "$OUT/trades.csv" ] || echo "empty" > "$OUT/trades.csv"
          # manifest + params snapshot
          echo "{\"run\":\"single\",\"ts\":\"$(date -Iseconds)\"}" > "$OUT/manifest.json"
          [ -f "$PF" ] && cp -f "$PF" "$OUT/params_used.yml" || true
          ( cd "$OUT" && zip -rq ../bundle_single.zip . )
      - name: Upload bundle_single.zip
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        with:
          name: bundle_single
          path: bundle_single.zip
          if-no-files-found: warn

  wfo:
    needs: prep
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      max-parallel: 4
      matrix: ${{ fromJson(needs.prep.outputs.matrix) }}
    steps:
      - name: Checkout (pinned)
        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8
      - name: Setup Python (pinned)
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065
        with:
          python-version: ${{ github.event.inputs.PY_VERSION }}
      - name: Deps
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install pandas numpy pyyaml
      - name: Unzip data if present
        run: |
          set -euo pipefail
          if [ -f "${{ github.event.inputs.DATA_ZIP }}" ]; then unzip -o "${{ github.event.inputs.DATA_ZIP }}" || true; fi
      - name: Preflight (auto-detect new/legacy)
        run: |
          set -euo pipefail
          PFS=""
          if [ -f "ci/preflight_strict.py" ]; then
            PFS="ci/preflight_strict.py"
          elif [ -f "preflight_strict.py" ]; then
            PFS="preflight_strict.py"
          fi
          if [ -n "${PFS}" ]; then
            if grep -q -- "--data-root" "${PFS}"; then
              mkdir -p _preflight_out
              python "${PFS}" \
                --data-root "${GITHUB_WORKSPACE}" \
                --csv-glob "${{ needs.prep.outputs.csv_glob }}" \
                --outdir "_preflight_out"
            else
              python "${PFS}" "${{ needs.prep.outputs.csv_glob }}"
            fi
          else
            : > _pre.py
            echo "import sys,glob,pandas as pd" >> _pre.py
            echo "g=sys.argv[1]" >> _pre.py
            echo "paths=glob.glob(g,recursive=True)" >> _pre.py
            echo "assert paths, f'No CSV found by glob: {g}'" >> _pre.py
            echo "df=pd.read_csv(paths[0],nrows=5)" >> _pre.py
            echo "req=['open_time','open','high','low','close','volume']" >> _pre.py
            echo "miss=[c for c in req if c not in df.columns]" >> _pre.py
            echo "assert not miss, f'Missing columns: {miss} in {paths[0]}'" >> _pre.py
            python _pre.py "${{ needs.prep.outputs.csv_glob }}"
          fi
      - name: Run WFO member
        env:
          THR: ${{ matrix.thr }}
          HOLD: ${{ matrix.hold }}
          FILTER: ${{ matrix.filter }}
          ENTRYPOINTS: ${{ github.event.inputs.ENTRYPOINTS }}
        run: |
          set -euo pipefail
          IFS='|' read -ra EPS <<< "${ENTRYPOINTS}"
          found=0
          for ep in "${EPS[@]}"; do
            if [ -f "$ep" ]; then
              set +e
              python "$ep"
              rc=$?
              set -e
              if [ $rc -eq 0 ]; then found=1; break; fi
            fi
          done
          if [ $found -eq 0 ]; then echo "Warning: no entrypoint succeeded" >&2; fi
      - name: HARVEST WFO -> standardize & zip
        env:
          THR: ${{ matrix.thr }}
          HOLD: ${{ matrix.hold }}
          FILTER: ${{ matrix.filter }}
          PF: ${{ needs.prep.outputs.params_used }}
        run: |
          set -euo pipefail
          OUT="out_${THR}_${HOLD}_${FILTER}"
          mkdir -p "$OUT"
          fcopy(){ p="$1"; dst="$2"; set +e; X=$(find . -type f -iname "$p" | head -n1); set -e; if [ -n "$X" ]; then cp -f "$X" "$OUT/$dst"; fi; }
          fcopy "trades.csv" "trades.csv"
          fcopy "*trade*.csv" "trades.csv"
          fcopy "summary.json" "summary.json"
          fcopy "*summary*.json" "summary.json"
          fcopy "preds_test.csv" "preds_test.csv"
          fcopy "*pred*.csv" "preds_test.csv"
          fcopy "gating_debug.json" "gating_debug.json"
          fcopy "*gating*.json" "gating_debug.json"
          [ -f "$OUT/summary.json" ] || echo "{}" > "$OUT/summary.json"
          [ -f "$OUT/gating_debug.json" ] || echo "{}" > "$OUT/gating_debug.json"
          [ -f "$OUT/preds_test.csv" ] || echo "empty" > "$OUT/preds_test.csv"
          [ -f "$OUT/trades.csv" ] || echo "empty" > "$OUT/trades.csv"
          echo "{\"thr\":$THR,\"hold\":$HOLD,\"filter\":\"$FILTER\",\"ts\":\"$(date -Iseconds)\"}" > "$OUT/manifest.json"
          [ -f "$PF" ] && cp -f "$PF" "$OUT/params_used.yml" || true
          ( cd "$OUT" && zip -rq "../bundle_${THR}_${HOLD}_${FILTER}.zip" . )
      - name: Upload WFO bundle
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        with:
          name: bundle_${{ matrix.thr }}_${{ matrix.hold }}_${{ matrix.filter }}
          path: bundle_${{ matrix.thr }}_${{ matrix.hold }}_${{ matrix.filter }}.zip
          if-no-files-found: warn

  finalize:
    needs: [wfo, single]
    runs-on: ubuntu-latest
    steps:
      - name: Setup Python (pinned)
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065
        with:
          python-version: "3.11"
      - name: Download all artifacts (pinned)
        uses: actions/download-artifact@634f93cb2916e3fdff6788551b99b062d0335ce0
        with:
          path: all_artifacts
      - name: Debug list (tree + sizes)
        run: |
          set -euo pipefail
          echo "==== all_artifacts tree ===="
          ls -R all_artifacts || true
          echo "============================"
          echo "ZIP files found:"
          find all_artifacts -type f -name "*.zip" -printf "%p (%k KB)\n" | sort || true
      - name: Aggregate & Pack FINAL ZIP
        run: |
          set -euo pipefail
          python - <<'PY'
          import os,zipfile,glob,json,csv,math
          import pandas as pd
          import numpy as np
          ROOT="all_artifacts"
          bundles=[]
          for p in glob.glob(os.path.join(ROOT,"**","*.zip"),recursive=True):
            bundles.append(p)
          # also consider unzipped dirs that already contain standard files
          std_dirs=[]
          for d in glob.glob(os.path.join(ROOT,"**"),recursive=True):
            if os.path.isdir(d):
              if all(os.path.exists(os.path.join(d,f)) for f in ["trades.csv","summary.json","preds_test.csv","gating_debug.json"]):
                std_dirs.append(d)
          os.makedirs("final_pack",exist_ok=True)
          rows=[]
          def safe_read_csv(p):
            try: return pd.read_csv(p)
            except: return pd.DataFrame()
          def compute_trade_metrics(df):
            n=len(df); r={"n_trades":n}
            if n==0: r.update({"win_rate":np.nan,"cum_pnl":0.0,"avg_pnl":np.nan,"median_pnl":np.nan}); return r
            pnl_col=None
            for c in ["pnl_close_based","pnl","pnl_value","pnl_usd","pnl_krw","pnl_pct","pnl_percent"]:
              if c in df.columns: pnl_col=c; break
            if pnl_col is None: r.update({"win_rate":np.nan,"cum_pnl":np.nan,"avg_pnl":np.nan,"median_pnl":np.nan}); return r
            pnl=pd.to_numeric(df[pnl_col],errors="coerce").fillna(0.0)
            r["cum_pnl"]=float(pnl.sum())
            r["avg_pnl"]=float(pnl.mean())
            r["median_pnl"]=float(pnl.median())
            r["win_rate"]=float((pnl>0).sum())/max(n,1)
            return r
          def compute_mcc(df):
            if df is None or df.empty: return {}
            m={c.lower():c for c in df.columns}
            yt=next((m[k] for k in ["y_true","true","label","target"] if k in m),None)
            yp=next((m[k] for k in ["y_pred","pred","prediction","pred_label"] if k in m),None)
            if not yt or not yp: return {}
            Yt=df[yt].values; Yp=df[yp].values
            if df[yp].dtype.kind in "f": Yp=(Yp>=0.5).astype(int)
            tp=int(((Yt==1)&(Yp==1)).sum()); tn=int(((Yt==0)&(Yp==0)).sum())
            fp=int(((Yt==0)&(Yp==1)).sum()); fn=int(((Yt==1)&(Yp==0)).sum())
            den=(tp+fp)*(tp+fn)*(tn+fp)*(tn+fn); den=math.sqrt(den) if den else 0
            mcc=(tp*tn - fp*fn)/den if den else float("nan")
            return {"mcc":float(mcc),"tp":tp,"tn":tn,"fp":fp,"fn":fn}
          # extract bundles (if any) into temp and aggregate
          TMP="tmp_extract"; os.makedirs(TMP,exist_ok=True)
          def harvest_dir(d, run_id):
            r={"run_id":run_id}
            t=os.path.join(d,"trades.csv"); s=os.path.join(d,"summary.json"); p=os.path.join(d,"preds_test.csv")
            if os.path.exists(t): r.update(compute_trade_metrics(safe_read_csv(t)))
            if os.path.exists(s):
              try:
                summ=json.load(open(s,"r",encoding="utf-8"))
                for k in ["entries","exits","cum_pnl_close_based","avg_gatep","sharpe","mdd","profit_factor","win_rate"]:
                  if k in summ and (k!="win_rate" or math.isnan(r.get("win_rate",float("nan")))):
                    r[k]=summ[k]
              except: pass
            if os.path.exists(p): r.update(compute_mcc(safe_read_csv(p)))
            rows.append(r)
          import re
          def infer_run_id(name):
            m=re.search(r'(?:bundle|out)[-_]([0-9.]+)[-_]([0-9]+)[-_]([A-Za-z0-9]+)',name)
            if m: return f"thr={m.group(1)},hold={m.group(2)},filter={m.group(3)}"
            if "single" in name: return "single"
            return name
          for zp in bundles:
            nm=os.path.splitext(os.path.basename(zp))[0]
            rid=infer_run_id(nm)
            od=os.path.join(TMP,nm); os.makedirs(od,exist_ok=True)
            try:
              with zipfile.ZipFile(zp,'r') as z: z.extractall(od)
              harvest_dir(od,rid)
            except: pass
          for d in std_dirs:
            harvest_dir(d,infer_run_id(os.path.basename(d)))
          # build df
          df=pd.DataFrame(rows)
          cols=["run_id","n_trades","win_rate","cum_pnl","avg_pnl","median_pnl","entries","exits","avg_gatep","profit_factor","sharpe","mdd","mcc","tp","tn","fp","fn"]
          for c in cols:
            if c not in df.columns: df[c]=np.nan
          df=df[cols]
          df["cum_pnl"]=pd.to_numeric(df["cum_pnl"],errors="coerce")
          df["win_rate"]=pd.to_numeric(df["win_rate"],errors="coerce")
          df["mcc"]=pd.to_numeric(df["mcc"],errors="coerce")
          df=df.sort_values(by=["cum_pnl","win_rate","mcc"],ascending=[False,False,False],na_position="last")
          df.to_csv("final_pack/WFO_aggregated_summary.csv",index=False)
          # pack everything into single zip
          FINAL=os.environ.get("BUNDLE","WFO_results_all.zip")
          with zipfile.ZipFile(FINAL,"w",zipfile.ZIP_DEFLATED) as z:
            # include all artifacts (dirs) and bundle zips
            for base,dirs,files in os.walk("all_artifacts"):
              for fn in files:
                p=os.path.join(base,fn)
                z.write(p, os.path.relpath(p,"all_artifacts"))
            # include summary and a tiny README
            z.write("final_pack/WFO_aggregated_summary.csv","WFO_aggregated_summary.csv")
            z.writestr("README.txt","This package includes all member bundles and WFO_aggregated_summary.csv\n")
          PY
      - name: Upload FINAL ZIP (pinned)
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        with:
          name: ${{ github.event.inputs.BUNDLE_NAME }}
          path: WFO_results_all.zip
          if-no-files-found: error
