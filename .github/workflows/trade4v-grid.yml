name: trade4v-grid
on:
  workflow_dispatch:
    inputs:
      project_zip:  { description: "project zip in repo", default: "trade4v.zip", required: true }
      data_file:    { description: "ETH 1m zip OR csv in repo", default: "ETHUSDT_1min_2020_2025.csv", required: true }
      train_range:  { description: "train_start,train_end", default: "2025-01-01,2025-04-30", required: true }
      test_range:   { description: "test_start,test_end",  default: "2025-05-01,2025-06-30",  required: true }
      params_pack:  { description: "fee;slip;size;regime;session;exp", default: "fee=10;slip=0;size=kelly,0.02,0.8;regime=64,1.0,2.0;session=AS:1.0,EU:1.0,US:1.0;exp=1,0.7,1.8", required: true }

jobs:
  grid:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@08eba0b27e820071cde6df949e0beb9ba4906955

      - name: Setup Python
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065
        with: { python-version: "3.11" }

      - name: Unzip project
        run: unzip -q "${{ github.event.inputs.project_zip }}" -d .

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r trade4v/requirements.txt

      - name: Resolve & normalize ETH 1m (zip or csv)
        shell: bash
        run: |
          set -euo pipefail
          DATA_IN="${{ github.event.inputs.data_file }}"
          if [[ -f "$DATA_IN" ]]; then
            FOUND="$DATA_IN"
          else
            FOUND=$(find . -maxdepth 2 -type f \( -iname "*.zip" -o -iname "*.csv" \) \
                     | grep -iE 'eth.*(1min|1m)|ethusdt.*(1min|1m)' \
                     | head -n 1 || true)
          fi
          if [[ -z "${FOUND:-}" ]]; then
            echo "❌ No data file found."; ls -la; exit 1
          fi
          echo "✅ Using data file: $FOUND"
          mkdir -p data
          if [[ "$FOUND" == *.zip ]]; then unzip -q "$FOUND" -d data; else cp "$FOUND" data/; fi
          python - <<'PY'
          import glob, pandas as pd, numpy as np
          srcs = sorted(glob.glob("data/*.csv"))
          assert srcs, "No CSV after extraction"
          df = pd.read_csv(srcs[0])
          df = df.rename(columns={c: c.strip().lower().replace(" ", "_") for c in df.columns})
          tcol = next((c for c in ["time","open_time","ts","timestamp"] if c in df.columns), None)
          if tcol is None: raise SystemExit("No time/open_time/ts/timestamp column")
          s = df[tcol]
          if np.issubdtype(s.dtype, np.number):
              med = float(s.iloc[len(s)//2]); unit = "ms" if med > 1e11 else "s"
              df["time"] = pd.to_datetime(s, unit=unit, utc=True)
          else:
              df["time"] = pd.to_datetime(s, utc=True)
          for c in ["open","high","low","close"]:
              assert c in df.columns, f"Missing {c}"
          if "volume" not in df.columns:
              cand = [c for c in ["vol","base_volume","quote_asset_volume","volume_usdt","volume_usd"] if c in df.columns]
              assert cand, "Missing volume"; df["volume"] = df[cand[0]]
          out = df[["time","open","high","low","close","volume"]].sort_values("time").drop_duplicates("time")
          out.to_csv("normalized.csv", index=False)
          PY
          echo "DATA_PATH=normalized.csv" >> $GITHUB_ENV

      - name: Parse ranges & params
        shell: bash
        run: |
          set -euo pipefail
          IFS=',' read -r TRAIN_S TRAIN_E <<< "${{ github.event.inputs.train_range }}"
          IFS=',' read -r TEST_S  TEST_E  <<< "${{ github.event.inputs.test_range }}"
          echo "TRAIN_S=$TRAIN_S" >> $GITHUB_ENV
          echo "TRAIN_E=$TRAIN_E" >> $GITHUB_ENV
          echo "TEST_S=$TEST_S"   >> $GITHUB_ENV
          echo "TEST_E=$TEST_E"   >> $GITHUB_ENV

          # Defaults
          FEE_BPS="10"; SLIP_BPS="0"
          SIZE_MODE="kelly"; RISK_BUDGET="0.02"; SIZE_MAX="0.8"
          REGIME="64,1.0,2.0"; SESSION_FEE_MULT="AS:1.0,EU:1.0,US:1.0"
          USE_EXP="1"; SCALE_CLIP="0.7,1.8"

          PARAMS="${{ github.event.inputs.params_pack }}"
          IFS=';' read -ra KV <<< "$PARAMS"
          for pair in "${KV[@]}"; do
            key="${pair%%=*}"; val="${pair#*=}"
            case "$key" in
              fee) FEE_BPS="$val" ;;
              slip) SLIP_BPS="$val" ;;
              size) IFS=',' read -r SIZE_MODE RISK_BUDGET SIZE_MAX <<< "$val" ;;
              regime) REGIME="$val" ;;
              session) SESSION_FEE_MULT="$val" ;;
              exp) IFS=',' read -r USE_EXP SCALE_LO SCALE_HI <<< "$val"; SCALE_CLIP="$SCALE_LO,$SCALE_HI" ;;
            esac
          done
          {
            echo "FEE_BPS=$FEE_BPS"
            echo "SLIP_BPS=$SLIP_BPS"
            echo "SIZE_MODE=$SIZE_MODE"
            echo "RISK_BUDGET=$RISK_BUDGET"
            echo "SIZE_MAX=$SIZE_MAX"
            echo "REGIME=$REGIME"
            echo "SESSION_FEE_MULT=$SESSION_FEE_MULT"
            echo "USE_EXP=$USE_EXP"
            echo "SCALE_CLIP=$SCALE_CLIP"
          } >> $GITHUB_ENV

      # =========================
      # Stage 1: 방향 멀티그리드
      # =========================
      - name: Run Stage-1 (Direction grid)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p _grid/S1
          # Grids (작게 빠르게)
          H_MAPS=("0:11,1:15,2:21" "0:13,1:17,2:25" "0:15,1:19,2:29")
          BAR_MAPS=("0:20,12,15;1:30,20,20;2:70,35,45" "0:20,12,15;1:40,25,25;2:90,45,60")
          EV_FLOORS=("1.0" "1.5")
          K_DAY="0:0,1:0,2:20"         # 방향 평가에선 K 고정
          SCALE_CLIP="0.7,1.8"
          USE_EXP="0"                  # 스케일 OFF (방향만 봄)
          RUN_ID=0
          for HMAP in "${H_MAPS[@]}"; do
            for BMAP in "${BAR_MAPS[@]}"; do
              for FLOOR in "${EV_FLOORS[@]}"; do
                RUN_ID=$((RUN_ID+1))
                OUT="_grid/S1/run_${RUN_ID}"
                mkdir -p "$OUT"
                echo "▶ S1 $RUN_ID  H=${HMAP}  BAR=${BMAP}  FLOOR=${FLOOR}"
                python trade4v/run_4u.py \
                  --data        "${DATA_PATH}" \
                  --train_start "${TRAIN_S}" --train_end "${TRAIN_E}" \
                  --test_start  "${TEST_S}"  --test_end  "${TEST_E}" \
                  --H 15 --H_map "${HMAP}" \
                  --fee_bps "${FEE_BPS}" --slip_bps "${SLIP_BPS}" \
                  --session_fee_mult "${SESSION_FEE_MULT}" \
                  --regime "${REGIME}" \
                  --bar_map "${BMAP}" \
                  --k_day  "${K_DAY}" \
                  --ev_anneal "${FLOOR}" \
                  --size_mode "${SIZE_MODE}" --risk_budget "${RISK_BUDGET}" --size_max "${SIZE_MAX}" \
                  --use_expansion "${USE_EXP}" --scale_clip "${SCALE_CLIP}" \
                  --out_dir "${OUT}" || true
              done
            done
          done

          # 요약 & 베스트 선택: 승률 최대(조건: pnl>=0, trades>=20)
          python - <<'PY'
          import json,glob,os,csv
          rows=[]; best=None
          for p in sorted(glob.glob("_grid/S1/run_*/metrics_oos.json")):
            with open(p) as f: m=json.load(f)
            hit=m.get("hit_rate",0.0); pnl=m.get("pnl",0.0); tr=m.get("trades",0)
            cfg=json.load(open(os.path.join(os.path.dirname(p),"config.json")))
            row=dict(run=os.path.dirname(p), hit_rate=hit, pnl=pnl, trades=tr,
                     H_map=cfg["H_map"], bar_map=cfg["bar_map"], ev_anneal=cfg["ev_anneal"],
                     k_day=cfg["k_day"])
            rows.append(row)
            ok = (pnl>=0.0) and (tr>=20)
            score = (hit if ok else 0.0)
            if (best is None) or (score>best[0]):
              best=(score,row)
          os.makedirs("_grid/S1",exist_ok=True)
          with open("_grid/S1/summary.csv","w",newline="") as f:
            w=csv.DictWriter(f,fieldnames=rows[0].keys()); w.writeheader(); w.writerows(rows)
          if best is None: best=(0.0, rows[0] if rows else {})
          import json as J
          with open("_grid/S1/best.json","w") as f: J.dump(best[1], f, indent=2)
          print("BEST_S1:",best[1])
          PY

      # =========================
      # Stage 2: 양(크기) 멀티그리드
      # =========================
      - name: Run Stage-2 (Amount grid; uses best of S1)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p _grid/S2
          # S1 베스트 불러오기
          H_MAP=$(jq -r '.H_map' _grid/S1/best.json)
          BAR_MAP=$(jq -r '.bar_map' _grid/S1/best.json)
          # 그리드 (양 튜닝)
          SCALE_CLIPS=("0.7,1.8" "0.7,2.0" "0.7,2.2")
          K_DAYS=("0:0,1:0,2:20" "0:0,1:0,2:30")
          EV_FLOORS=("1.0" "1.5")
          USE_EXP="1"
          RUN_ID=0
          for SCL in "${SCALE_CLIPS[@]}"; do
            for KD in "${K_DAYS[@]}"; do
              for FLOOR in "${EV_FLOORS[@]}"; do
                RUN_ID=$((RUN_ID+1))
                OUT="_grid/S2/run_${RUN_ID}"
                mkdir -p "$OUT"
                echo "▶ S2 $RUN_ID  SCALE=${SCL}  K=${KD}  FLOOR=${FLOOR}"
                python trade4v/run_4u.py \
                  --data        "${DATA_PATH}" \
                  --train_start "${TRAIN_S}" --train_end "${TRAIN_E}" \
                  --test_start  "${TEST_S}"  --test_end  "${TEST_E}" \
                  --H 15 --H_map "${H_MAP}" \
                  --fee_bps "${FEE_BPS}" --slip_bps "${SLIP_BPS}" \
                  --session_fee_mult "${SESSION_FEE_MULT}" \
                  --regime "${REGIME}" \
                  --bar_map "${BAR_MAP}" \
                  --k_day  "${KD}" \
                  --ev_anneal "${FLOOR}" \
                  --size_mode "${SIZE_MODE}" --risk_budget "${RISK_BUDGET}" --size_max "${SIZE_MAX}" \
                  --use_expansion "${USE_EXP}" --scale_clip "${SCL}" \
                  --out_dir "${OUT}" || true
              done
            done
          done

          # 요약 & 베스트 선택:
          # 효율 = pnl / sum(ev_final of selected); 기준: 효율>=0.7 우선, 다음 pnl 최대
          python - <<'PY'
          import os,glob,json,csv,pandas as pd
          rows=[]
          best=None
          for p in sorted(glob.glob("_grid/S2/run_*/metrics_oos.json")):
            d=os.path.dirname(p)
            m=json.load(open(p))
            try:
              preds=pd.read_csv(os.path.join(d,"preds_test.csv"))
              sel=preds.query("entry_flag==1")
              sum_ev=float(sel["ev_final"].sum()) if len(sel) else 0.0
            except Exception:
              sum_ev=0.0
            pnl=m.get("pnl",0.0); tr=m.get("trades",0); hit=m.get("hit_rate",0.0)
            eff=(pnl/sum_ev) if sum_ev>0 else 0.0
            cfg=json.load(open(os.path.join(d,"config.json")))
            row=dict(run=d, pnl=pnl, trades=tr, hit_rate=hit, eff=eff,
                     H_map=cfg["H_map"], bar_map=cfg["bar_map"], ev_anneal=cfg["ev_anneal"],
                     k_day=cfg["k_day"], scale_clip=cfg["scale_clip"])
            rows.append(row)
            # 선택 규칙: 효율>=0.7을 우선, 그 다음 pnl 최대
            score = (1000.0 if eff>=0.7 else 0.0) + pnl
            if (best is None) or (score>best[0]):
              best=(score,row)
          os.makedirs("_grid/S2",exist_ok=True)
          import csv as C
          with open("_grid/S2/summary.csv","w",newline="") as f:
            w=C.DictWriter(f,fieldnames=rows[0].keys()); w.writeheader(); w.writerows(rows)
          import json as J
          with open("_grid/S2/best.json","w") as f: J.dump(best[1] if best else {}, f, indent=2)
          print("BEST_S2:",best[1] if best else None)
          PY

      - name: Upload grid outputs
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        with:
          name: grid-results
          path: |
            _grid/S1/summary.csv
            _grid/S1/best.json
            _grid/S2/summary.csv
            _grid/S2/best.json
